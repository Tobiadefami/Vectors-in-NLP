# Word Vectors in NLP

Words are represented as vectors consisting of numbers in natural language processing. The vector encodes the meaning of the word. 
These numbers (or weights) for each word are learned using various machine learning models, 

This notebook contains the following:

- Predicting analogies between words.
- The use of PCA to reduce the dimensionality of the word embeddings and plot them in two dimensions.
- Comparing word embeddings by using a similarity measure (the cosine similarity).
- An understanding of how these vector space models work.
